{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def grad_U(Ui, Yij, Vj, ai, bj, reg, eta, mu):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * (reg * Ui - Vj*((Yij - (mu + np.dot(Ui, Vj) + ai + bj))))\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, ai, bj, reg, eta, mu):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * (reg * Vj - Ui*((Yij - (mu + np.dot(Ui, Vj) + ai + bj))))\n",
    "\n",
    "def grad_a(Ui, Yij, Vj, ai,bj, reg, eta, mu):\n",
    "    \"\"\"Calculate the bias term for U\"\"\"\n",
    "    return eta * (reg * ai - (Yij - (mu + np.dot(Ui, Vj) + ai + bj)))\n",
    "\n",
    "def grad_b(Ui, Yij, Vj, ai, bj, reg, eta, mu):\n",
    "    \"\"\"Calculate the bias term for V\"\"\"\n",
    "    return eta * (reg * bj - (Yij - (mu + np.dot(Ui, Vj) + ai + bj)))\n",
    "\n",
    "def get_err(U, V, Y, a, b, mu, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for (user, movie, rating) in Y:\n",
    "        err = (rating - (mu + np.dot(U[user - 1], V[movie - 1]) + a[user - 1] + b[movie - 1]))**2\n",
    "        errors.append(err)\n",
    "    return  np.sqrt(sum(errors))/len(Y)\n",
    "\n",
    "    \n",
    "def train_model_with_bias(M, N, K, eta, reg, mu,  Y, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the U and V matrices to random\n",
    "    # values in [-0.5, 0.5]\n",
    "    U = np.random.rand(M, K) - 0.5\n",
    "    V = np.random.rand(N, K) - 0.5\n",
    "    \n",
    "    # Have one ai for every user, and one vj for every movie\n",
    "    a = np.random.rand(M, 1) - 0.5\n",
    "    b = np.random.rand(N, 1) - 0.5\n",
    "    \n",
    "    delta_01 = None\n",
    "    delta = None\n",
    "    n = 0\n",
    "    prev_loss = get_err(U, V, Y, a, b, mu, reg)\n",
    "    \n",
    "    print(prev_loss)\n",
    "    # Run for input number of epochs\n",
    "    while n < max_epochs:\n",
    "        # Implement early stopping\n",
    "        if delta_01 != None and delta != None and delta < eps * delta_01:\n",
    "            break\n",
    "        # Shuffle X and Y in order to add stochasticity\n",
    "        p = np.random.permutation(len(Y))\n",
    "        Y = Y[p]\n",
    "        # In each epoch, iterate through all the points and\n",
    "        # update the U and V vectors\n",
    "        # Need to update the U and V simultaneously, so need to create a temp variable.\n",
    "        for (user, movie, rating) in Y:\n",
    "            gradu = grad_U(U[user - 1], rating, V[movie - 1], a[user - 1], b[movie - 1], reg, eta, mu)\n",
    "            gradv = grad_V(V[movie - 1], rating, U[user - 1], a[user - 1], b[movie - 1], reg, eta, mu)\n",
    "            grada = grad_a(U[user - 1], rating, V[movie - 1], a[user - 1], b[movie - 1], reg, eta, mu)\n",
    "            gradb = grad_b(U[user - 1], rating, V[movie - 1], a[user - 1], b[movie - 1], reg, eta, mu)         \n",
    "            U[user - 1] = U[user - 1] - gradu\n",
    "            V[movie - 1] = V[movie - 1] - gradv\n",
    "            a[user - 1] = a[user -1] - grada\n",
    "            b[movie - 1] = b[movie -1] - gradb             \n",
    "        if n == 0:\n",
    "            curr_loss = get_err(U, V,Y, a, b, mu, reg)\n",
    "            delta_01 = prev_loss - curr_loss\n",
    "            prev_loss = curr_loss\n",
    "        else:\n",
    "            curr_loss = get_err(U, V,Y, a, b, mu, reg)\n",
    "            print(curr_loss)\n",
    "            delta = prev_loss - curr_loss\n",
    "            prev_loss = curr_loss\n",
    "        n += 1\n",
    "    print(n)\n",
    "    final_mse = get_err(U, V, Y, a,b, mu)\n",
    "    return (U, V, a, b, final_mse)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rahil's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    eta (the learning rate), ai (ith element of a), bj (jth element of b)\n",
    "    and mu (average rating).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * ((reg * Ui) - Vj * (Yij - mu - np.dot(Ui, Vj) - ai - bj))\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, reg, eta, ai, bj, mu):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    eta (the learning rate), ai (ith element of a), bj (jth element of b)\n",
    "    and mu (average rating).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * ((reg * Vj) - Ui * (Yij - mu - np.dot(Ui, Vj) - ai - bj))\n",
    "\n",
    "def grad_a(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    eta (the learning rate), ai (ith element of a), bj (jth element of b)\n",
    "    and mu (average rating).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to ai multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * ((reg * ai) - (Yij - mu - np.dot(Ui, Vj) - ai - bj))\n",
    "\n",
    "def grad_b(Ui, Yij, Vj, reg, eta, ai, bj, mu):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    eta (the learning rate), ai (ith element of a), bj (jth element of b)\n",
    "    and mu (average rating).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to bj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta * ((reg * bj) - (Yij - mu - np.dot(Ui, Vj) - ai - bj))\n",
    "\n",
    "def get_err(U, V, Y, a, b, mu, reg=0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j,\n",
    "    user/movie matrices U and V, user/movie vectors a and b and average rating mu.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "\n",
    "    reg_err = (reg/2) * (np.linalg.norm(U) ** 2 + np.linalg.norm(V) ** 2 +\n",
    "                         np.linalg.norm(a) ** 2 + np.linalg.norm(b) ** 2)\n",
    "    loss_err = 0\n",
    "    for point in Y:\n",
    "        i, j, y = point\n",
    "        loss_err += (y - mu - np.dot(U[i - 1], V[j - 1]) - a[i - 1] - b[j - 1]) ** 2\n",
    "    return ((reg_err + (1/2) * loss_err)/len(Y))\n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, mu, eps=0.0001, max_epochs=300):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V and M x 1 vector a and N x 1 vector b\n",
    "    such that rating Y_ij is approximated by (UV^T)_ij + a_i + b_j + mu.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, a, b, err) consisting of U, V, a, b and the\n",
    "    unregularized MSE of the model.\n",
    "    \"\"\"\n",
    "    # Randomly initialize U, V, a and b.\n",
    "    U = np.random.uniform(-0.5, 0.5, (M, K))\n",
    "    V = np.random.uniform(-0.5, 0.5, (N, K))\n",
    "    a = np.random.uniform(-0.5, 0.5, (M, ))\n",
    "    b = np.random.uniform(-0.5, 0.5, (N, ))\n",
    "    # Find the initial loss.\n",
    "    prev_err = get_err(U, V, Y, a, b, mu, reg)\n",
    "    for epoch in range(max_epochs):\n",
    "        np.random.shuffle(Y)\n",
    "        for point in Y:\n",
    "            # Update the rows of U and V and elements of a and b.\n",
    "            i, j, y = point\n",
    "            del_U = grad_U(U[i - 1], y, V[j - 1], reg, eta, a[i - 1], b[j - 1], mu)\n",
    "            del_V = grad_V(V[j - 1], y, U[i - 1], reg, eta, a[i - 1], b[j - 1], mu)\n",
    "            del_a = grad_a(U[i - 1], y, V[j - 1], reg, eta, a[i - 1], b[j - 1], mu)\n",
    "            del_b = grad_b(U[i - 1], y, V[j - 1], reg, eta, a[i - 1], b[j - 1], mu)\n",
    "            U[i - 1] = U[i - 1] - del_U\n",
    "            V[j - 1] = V[j - 1] - del_V\n",
    "            a[i - 1] = a[i - 1] - del_a\n",
    "            b[j - 1] = b[j - 1] - del_b\n",
    "        err = get_err(U, V, Y, a, b, mu, reg)\n",
    "        diff = prev_err - err\n",
    "        if epoch == 0:\n",
    "            # Compute the initial loss reduction.\n",
    "            tol = diff\n",
    "        else:\n",
    "            # Check if loss reduction is too small.\n",
    "            if diff/tol <= eps:\n",
    "                return(U, V, a, b, get_err(U, V, Y, a, b, mu))\n",
    "        prev_err = err\n",
    "        print(err)\n",
    "    return(U, V, a, b, np.sqrt(get_err(U, V, Y, a , b, mu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('um/all.dta').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,    185,   2160,      1],\n",
       "       [     1,    490,   2160,      4],\n",
       "       [     1,   1811,   2160,      3],\n",
       "       ...,\n",
       "       [458293,  17567,   1796,      1],\n",
       "       [458293,  17604,   1745,      3],\n",
       "       [458293,  17689,   1715,      3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.loadtxt('um/all.idx').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probe = []\n",
    "y_base = []\n",
    "y_valid = []\n",
    "y_hidden = []\n",
    "y_qual = []\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    if idx == 1:\n",
    "        y_base.append(train[i])\n",
    "    elif idx == 2:\n",
    "        y_valid.append(train[i])\n",
    "    elif idx == 3:\n",
    "        y_hidden.append(train[i])\n",
    "    elif idx == 4:\n",
    "        y_probe.append(train[i])\n",
    "    else:\n",
    "        y_qual.append(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = max(train[:,0]).astype(int) # users\n",
    "N = max(train[:,1]).astype(int) # movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing with  458293  users,  17770  movies.\n",
      "0.4303618358279184\n",
      "0.42500752150348386\n",
      "0.41822624282231\n",
      "0.4080197699046086\n",
      "0.4023697310535872\n",
      "0.3992372313614591\n",
      "0.3972310833965126\n",
      "0.3952017050429451\n",
      "0.39377805120210263\n",
      "0.392328884701505\n",
      "0.3912331749600694\n",
      "0.38994089637498547\n",
      "0.3887248371934991\n"
     ]
    }
   ],
   "source": [
    "print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "k = 20\n",
    "reg = 0.1\n",
    "eta = 0.01 # learning rate\n",
    "mu = np.mean(np.array(y_base + y_valid + y_hidden)[:, 3])\n",
    "#U, V, a, b, rmse = train_model_with_bias(M, N, k, eta, reg, mu, np.array(y_base + y_valid + y_hidden + y_probe)[:, [0, 1, 3]])\n",
    "U, V, a, b, rmse = train_model(M, N, k, eta, reg, np.array(y_base + y_valid + y_hidden)[:, [0, 1, 3]], mu)\n",
    "\n",
    "V = V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error is  [0.00081946]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test error is \", get_err(U, V.T,np.array(y_probe)[:, [0, 1, 3]], a, b, mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for (user, movie, rating) in np.array(y_qual)[:, [0, 1, 3]]:\n",
    "    test_preds.append((mu + np.dot(U[user - 1], V.T[movie - 1]) + a[user - 1] + b[movie - 1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749898"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_preds2.txt', 'w') as f:\n",
    "    for item in test_preds:\n",
    "        f.write(\"%.4f\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
